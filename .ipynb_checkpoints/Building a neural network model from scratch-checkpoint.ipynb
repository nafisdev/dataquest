{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Building A Handwritten Digits Classifier\n",
    "---\n",
    "In this Guided Project, we'll explore the effectiveness of deep, feedforward neural networks at classifying images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn contains a number of datasets pre-loaded with the library, within the namespace of sklearn.datasets. \n",
    "# The load_digits() function returns a copy of the hand-written digits dataset from UCI.\n",
    "from sklearn.datasets import load_digits\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in dataset as Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that contains the handwritten_digits features and a series that contains corresponding labels\n",
    "handwritten_digits_features, handwritten_digits_labels = load_digits(return_X_y= True)\n",
    "handwritten_digits_features = pd.DataFrame(handwritten_digits_features)\n",
    "handwritten_digits_labels = pd.Series(handwritten_digits_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(    0    1    2     3     4     5    6    7    8    9  ...    54   55   56  \\\n",
       " 0  0.0  0.0  5.0  13.0   9.0   1.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
       " 1  0.0  0.0  0.0  12.0  13.0   5.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
       " 2  0.0  0.0  0.0   4.0  15.0  12.0  0.0  0.0  0.0  0.0 ...   5.0  0.0  0.0   \n",
       " 3  0.0  0.0  7.0  15.0  13.0   1.0  0.0  0.0  0.0  8.0 ...   9.0  0.0  0.0   \n",
       " 4  0.0  0.0  0.0   1.0  11.0   0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
       " \n",
       "     57   58    59    60    61   62   63  \n",
       " 0  0.0  6.0  13.0  10.0   0.0  0.0  0.0  \n",
       " 1  0.0  0.0  11.0  16.0  10.0  0.0  0.0  \n",
       " 2  0.0  0.0   3.0  11.0  16.0  9.0  0.0  \n",
       " 3  0.0  7.0  13.0  13.0   9.0  0.0  0.0  \n",
       " 4  0.0  0.0   2.0  16.0   4.0  0.0  0.0  \n",
       " \n",
       " [5 rows x 64 columns], 0    0\n",
       " 1    1\n",
       " 2    2\n",
       " 3    3\n",
       " 4    4\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handwritten_digits_features.head(), handwritten_digits_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the handwritten_digits_features we see that each image (as each row) has 64 grayscale points, which means they were 8\\*8 images. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize a few sample data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALUAAAC1CAYAAAAZU76pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npNk6HwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1936d31a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Transform row 0, 100, 200, 300, 1000, 1100, 1200, 1300 to 8*8 pixel grid and visualize the data\n",
    "rows = [0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500]\n",
    "fig = plt.figure(figsize = (3, 3))\n",
    "i = 1\n",
    "for row in rows:\n",
    "    # Transform selected datapoint to np array then reshape to original grid\n",
    "    img = handwritten_digits_features.iloc[row].values.reshape(8, 8) \n",
    "    ax = fig.add_subplot(4, 4, i)\n",
    "    ax.imshow(img, cmap = 'gray_r')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    i+=1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training k-nearest neighbors models.\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Performs 4-fold cross validation using k-nearest neighbors\n",
    "def cross_validate(features, labels, n):\n",
    "    kf = KFold(n_splits = 4, random_state = 1)\n",
    "    scores = []\n",
    "\n",
    "    # Split data\n",
    "    for train_idx, test_idx in kf.split(features, labels):\n",
    "        X_train, X_test = features.loc[train_idx], features.loc[test_idx]\n",
    "        y_train, y_test = labels.loc[train_idx], labels.loc[test_idx]\n",
    "        # Train & predict with each fold \n",
    "        knn = KNeighborsClassifier(n_neighbors = n)\n",
    "        knn.fit(X_train, y_train)\n",
    "        predictions = knn.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, predictions, normalize = True)\n",
    "        scores.append(accuracy)\n",
    "    return scores    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment average recall score with different k(s)\n",
    "k_num = range(1, 16)\n",
    "avg_accuracy = []\n",
    "for n in k_num:\n",
    "    scores = cross_validate(handwritten_digits_features, handwritten_digits_labels, n)\n",
    "    avg_accuracy.append(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 15)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD7CAYAAACG50QgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nir6IiB9R0RcR8SMq+iIifkRFX0TEj/x/XZYf79sOCmwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1936d77f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize K, recall score correlation\n",
    "plt.plot(k_num, avg_accuracy)\n",
    "plt.xlim(1, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**While the accuracy score is not bad, there are a few downsides to using k-nearest neighbors:**\n",
    "* high memory usage (for each new unseen observation, many comparisons need to be made to seen observations)\n",
    "* no model representation to debug and explore\n",
    "\n",
    "---\n",
    "\n",
    "Let's now try a neural network with a single hidden layer. Use the MLPClassifier package from scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Build a Neural Network model from scratch \n",
    "*** -- based on guides from [Coursera Machine Learning course by Andrew Ng from Stanford](https://www.coursera.org/learn/machine-learning?) ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split for train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(handwritten_digits_features, \n",
    "                                                    handwritten_digits_labels, \n",
    "                                                   test_size = 0.2,\n",
    "                                                   random_state = 1)\n",
    "\n",
    "# Split for final result of train(60%) and val(20%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
    "                                                 test_size = 0.25, \n",
    "                                                 random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized Sigmoid function\n",
    "def sigmoid(X):\n",
    "    return 1/(1+np.exp(-X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomly initialize the parameters for symmetry breaking\n",
    "---\n",
    "When training neural networks, it is important to randomly initialize the parameters for symmetry breaking. One effective strategy for random initialization is to randomly select values for $\\Theta^{l}$ uniformly in the range $[-\\epsilon_{init}, \\epsilon_{init}]$.<sup>1, 2</sup> This range of values ensures that the parameters are kept small and makes the learning more efficient.\n",
    "\n",
    "---\n",
    "1. One effective strategy for choosing Îµ_init is to base it on the number of units in the\n",
    "network. A good choice of $ \\epsilon_{init} $ is  $ \\epsilon_{init}  = \\frac{\\sqrt{6}}{\\sqrt{L_{in}+L_{out}}} $, where $L_{in} = s_{l} $ and $L_{out}= s_{l+1}$ are\n",
    "the number of units(neurons) in the layers adjacent to $\\Theta^{l}$.\n",
    "<br><br>\n",
    "2. For $ x\\in[0, 1] $, $x\\times2\\times\\epsilon_{init}-\\epsilon_{init}\\in[-\\epsilon_{init}, \\epsilon_{init}] $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomly initialize weights with parameter hidden_layer_size in list datatype\n",
    "# Returns the same weights in matrix and vector form\n",
    "\n",
    "def rand_weights_init(X, n_labels, hidden_layer_sizes):\n",
    "    # Number of neurons in input layer\n",
    "    input_layer_size = X.shape[1]\n",
    "    \n",
    "    # Compute number of neurons in all layers in the nueral network model \n",
    "    all_layers = [input_layer_size] \n",
    "    all_layers = all_layers+hidden_layer_sizes+ [n_labels]\n",
    "\n",
    "    n_layers = len(all_layers)\n",
    "    \n",
    "    # Initialize weights for all layers \n",
    "    weights = []\n",
    "    \n",
    "    # Initialize unrolled weights for later use\n",
    "    weights_vec = []\n",
    "    \n",
    "    for n in range(n_layers-1):\n",
    "        in_num = all_layers[n]\n",
    "        out_num = all_layers[n+1]\n",
    "        \n",
    "        # Initialize epsilon\n",
    "        epsilon_init = np.log(6)/np.log(in_num + out_num)\n",
    " \n",
    "        np.random.seed(0)\n",
    "        # w is set to a matrix of size(out_num, 1 + in_num) as the first column of w handles the \"bias\" terms\n",
    "        w = np.random.rand(out_num, 1 + in_num) * 2 * epsilon_init - epsilon_init\n",
    "        \n",
    "        # Unroll w for later use in fmin_cg function to minimize cost\n",
    "        w_vec = np.concatenate(w)\n",
    "        \n",
    "        weights.append(w)\n",
    "        weights_vec.append(w_vec)\n",
    "        \n",
    "    print('Number of weights in matrix: ', len(weights))    \n",
    "                     \n",
    "    return np.asarray(weights), np.concatenate(np.asarray(weights_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of weights in matrix:  2\n",
      "Theta1 shape:  (8, 65) \n",
      " Theta2 shape:  (10, 9)\n",
      "Unrolled weights:  (610,)\n"
     ]
    }
   ],
   "source": [
    "# Test run with weights initialization: 1 hidden layer with 8 neurons\n",
    "weights, weights_vector = rand_weights_init(X_train, 10, [8])\n",
    "print('Theta1 shape: ', weights[0].shape, '\\n','Theta2 shape: ', weights[1].shape)\n",
    "print('Unrolled weights: ', weights_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of weights in matrix:  3\n",
      "Theta1 shape:  (8, 65) \n",
      " Theta2 shape:  (8, 9) \n",
      " Theta3 shape:  (10, 9)\n",
      "Unrolled weights:  (682,)\n"
     ]
    }
   ],
   "source": [
    "# Test run with weights initialization: 2 hidden layer both with 8 neurons\n",
    "weights2, weights_vector2 = rand_weights_init(X_train, 10, [8, 8])\n",
    "print('Theta1 shape: ', weights2[0].shape, \n",
    "      '\\n','Theta2 shape: ', weights2[1].shape, \n",
    "      '\\n','Theta3 shape: ', weights2[2].shape)\n",
    "print('Unrolled weights: ', weights_vector2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularized Cost function of Neural Network with Sigmoid function\n",
    "---\n",
    "The cost function for the neural network without regularization is:\n",
    "$ J(\\theta)= \\frac{1}{m}\\sum\\limits_{i=1}^m\\sum\\limits_{k=1}^K[ây^{(i)}_{k}log((h_\\theta(x^{(i)}))_k)â(1ây^{(i)}_{k})log(1â(h_\\theta(x^{(i)}))_k)] $, where $m$ is the number of samples, $K$ is the number of labels, $h_\\theta(x)$ is the hypothesis, in our case the Sigmoid function.\n",
    "<br><br>\n",
    "\n",
    "The regularized cost function adds the regularization terms:\n",
    "$ \\frac{\\lambda}{2m}\\sum\\limits_{l = 1}^{L-1}\\sum\\limits_{no=1}^{N_{out}}\\sum\\limits_{ni=1}^{N_{in}}(\\Theta^{(l)}_{no, ni})^{2} $, where L is the total number of layers in the model, $N_{in}$ and $N_{out}$ are\n",
    "the number of units(neurons) in the layers adjacent to $\\Theta^{l}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unregularized cost function\n",
    "def cost_func(y_prob, y_actual, sample_size):\n",
    "\n",
    "    # Convert y_actual into dummy/indicator variables\n",
    "    y = y_actual.astype('category').copy()\n",
    "    \n",
    "    \"\"\"\n",
    "    Note to specify dtype when converting categorical dataframe to numpy array \n",
    "    If dtype not specified, -y will be operated on string type where if y = 1, -y = 255\n",
    "    \"\"\"\n",
    "    y = np.asarray(pd.get_dummies(y_actual), dtype = np.int64)\n",
    "\n",
    "#     # Convert y_pred into dummy/indicator variables \n",
    "#     # Step 1: Initialize y_pred dummie form with all zeros    \n",
    "#     h = np.zeros(y.shape)\n",
    "    \n",
    "#     # Step 2: Replace zeros with ones using y_pred as indices \n",
    "#     rows = np.arange(h.shape[0])\n",
    "#     h[rows, y_pred] = 1\n",
    "    \n",
    "    # Compute cost \n",
    "    cost = np.sum(-y*np.log10(y_prob)-(1-y)*np.log10(1-y_prob))/sample_size\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feedforward with Regularized Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Feedforward using sigmoid function \n",
    "def feedforward(vec_weights, hidden_layer_sizes, n_labels, X, y, alpha):\n",
    "    \n",
    "    # Define some useful variables\n",
    "    n_samples = X.shape[0]\n",
    "    input_layer_size = X.shape[1]\n",
    "    \n",
    "    # Compute number of neurons in all layers in the nueral network model     \n",
    "    all_layers = [input_layer_size] \n",
    "    all_layers = all_layers+hidden_layer_sizes+ [n_labels]\n",
    "\n",
    "#     print(all_layers)\n",
    "    \n",
    "    n_activation = len(all_layers)-1\n",
    "   \n",
    "\n",
    "    # Initialize input \n",
    "    x = np.asarray(X)\n",
    "    \n",
    "#     # Initialize output\n",
    "#     predictions = np.empty((n_samples, n_labels))\n",
    "\n",
    "    # Initialize a list of weights in matrix form \n",
    "    weights_matrix = []\n",
    "    \n",
    "    # Initialize weights without bias for regularization term\n",
    "    weights_no_bias = []\n",
    "    \n",
    "    # Initialize activaitons x for calculating 'error term' in backpropagation later\n",
    "    activations_x = []\n",
    "    \n",
    "    # Feedforward \n",
    "    for n in range(n_activation):\n",
    "        # Add bias column \n",
    "        x = np.insert(x, 0, 1, axis = 1)\n",
    "        \n",
    "        # Append activation x to activations, last activation is also the output\n",
    "        activations_x.append(x)\n",
    "        \n",
    "        # Reshape weights vector to weights matrix based on neural network structure\n",
    "        init_slice = 0\n",
    "        weights_count = (all_layers[n]+1)*all_layers[n+1]\n",
    "        weights = np.reshape(vec_weights[init_slice:(init_slice + weights_count)], (all_layers[n+1], (all_layers[n]+1)))\n",
    "        \n",
    "        # Append weights to the list of weights in matrix form \n",
    "        weights_matrix.append(weights)\n",
    "        \n",
    "        # Compute activation\n",
    "        activation = sigmoid(np.dot(x, weights.T))  \n",
    "        \n",
    "        # Assign output as the input for next layer, or as final output if loop ends \n",
    "        x = activation\n",
    "        \n",
    "        # Get weights without bias in each layerfor later calculating regularization term\n",
    "        no_bias= np.delete(weights, 0, axis = 1)\n",
    "        \n",
    "        # Unroll weights in each layer into a numpy 1d array & append to weights_no_bias\n",
    "        weights_no_bias.append(np.concatenate(no_bias))\n",
    "        \n",
    "        # Update init_slice \n",
    "        init_slice = weights_count\n",
    "        \n",
    "    # Sanity check: x as output should match the shape of predefined p\n",
    "    y_prob = x.copy()\n",
    "\n",
    "    # Assign index of the max value of each row as predictions \n",
    "    y_pred = np.argmax(y_prob, axis = 1)\n",
    "    \n",
    "    # Compute unregularized cost \n",
    "    cost = cost_func(y_prob, y, n_samples)\n",
    "    \n",
    "    # Unroll weights into a numpy 1d array\n",
    "    weights_no_bias = np.concatenate(np.array(weights_no_bias))\n",
    "    \n",
    "    # Regularization term \n",
    "    reg_term = np.sum(weights_no_bias**2)*alpha/(2*n_samples)\n",
    "    \n",
    "    # Compute regularized cost \n",
    "    reg_cost = cost + reg_term\n",
    "    \n",
    "    return reg_cost, y_prob, y_pred, activations_x, weights_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With alpha 0, the regularized cost is:  2.945120500303429\n",
      "*******************************************************\n",
      "With alpha 1, the regularized cost is:  2.9614436955038648\n",
      "*******************************************************\n",
      "With alpha 2, the regularized cost is:  2.9777668907043\n",
      "*******************************************************\n",
      "With alpha 3, the regularized cost is:  2.9940900859047357\n",
      "*******************************************************\n",
      "With alpha 4, the regularized cost is:  3.0104132811051714\n",
      "*******************************************************\n",
      "Probabilities shape:  (1077, 10)\n",
      "Predictions shape:  (1077,) predictions:  [4 4 2 ... 4 0 2]\n",
      "Activations number: 2\n",
      "Activation x1 shape (1077, 65)\n",
      "Activation x2 shape (1077, 9)\n",
      "weights 1 shape (8, 65)\n",
      "weights 2 shape (10, 9)\n"
     ]
    }
   ],
   "source": [
    "# Test feedforward with weights initialization: \n",
    "# 1 hidden layer with 8 neurons \n",
    "# from rand_weights_init test run & alpha range(5)\n",
    "\n",
    "for a in range(5):\n",
    "    reg_cost1, probabilties1, predictions1, activations_x1, weights_matrix1 = feedforward( \n",
    "                                        vec_weights = weights_vector, \n",
    "                                        n_labels=10, \n",
    "                                        X=X_train, y = y_train,\n",
    "                                        hidden_layer_sizes = [8],\n",
    "                                        alpha = a)\n",
    "    print('With alpha {}, the regularized cost is: '.format(a), reg_cost1)\n",
    "    print('*'*55)\n",
    "\n",
    "print('Probabilities shape: ', probabilties1.shape)\n",
    "print('Predictions shape: ', predictions1.shape, 'predictions: ', predictions1)\n",
    "print('Activations number:', len(activations_x1))\n",
    "print('Activation x1 shape', activations_x1[0].shape)\n",
    "print('Activation x2 shape', activations_x1[1].shape)\n",
    "print('weights 1 shape', weights_matrix1[0].shape)\n",
    "print('weights 2 shape', weights_matrix1[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With alpha 0, the regularized cost is:  3.0412919452900082\n",
      "*******************************************************\n",
      "With alpha 1, the regularized cost is:  3.059339249288558\n",
      "*******************************************************\n",
      "With alpha 2, the regularized cost is:  3.077386553287108\n",
      "*******************************************************\n",
      "With alpha 3, the regularized cost is:  3.0954338572856575\n",
      "*******************************************************\n",
      "With alpha 4, the regularized cost is:  3.1134811612842075\n",
      "*******************************************************\n",
      "Probabilities shape:  (1077, 10)\n",
      "Predictions shape:  (1077,) predictions:  [2 2 2 ... 2 2 2]\n",
      "Activations x number: 3\n",
      "Activation x1 shape (1077, 65)\n",
      "Activation x2 shape (1077, 9)\n",
      "Activation x3 shape (1077, 9)\n",
      "weights 1 shape (8, 65)\n",
      "weights 2 shape (8, 9)\n",
      "weights 3 shape (10, 9)\n"
     ]
    }
   ],
   "source": [
    "# Test feedforward with weights initialization: \n",
    "# 2 hidden layer both with 8 neurons \n",
    "# from rand_weights_init test run & alpha range(5)\n",
    "\n",
    "for a in range(5):\n",
    "    reg_cost2, probabilties2, predictions2, activations_x2, weights_matrix2 = feedforward( \n",
    "                                        vec_weights = weights_vector2, \n",
    "                                        hidden_layer_sizes = [8, 8],\n",
    "                                        n_labels=10, \n",
    "                                        X=X_train, y = y_train,\n",
    "                                        alpha = a)\n",
    "    print('With alpha {}, the regularized cost is: '.format(a), reg_cost2)\n",
    "    print('*'*55)\n",
    "\n",
    "print('Probabilities shape: ', probabilties2.shape)\n",
    "print('Predictions shape: ', predictions2.shape, 'predictions: ', predictions2)\n",
    "print('Activations x number:', len(activations_x2))\n",
    "print('Activation x1 shape', activations_x2[0].shape)\n",
    "print('Activation x2 shape', activations_x2[1].shape)\n",
    "print('Activation x3 shape', activations_x2[2].shape)\n",
    "print('weights 1 shape', weights_matrix2[0].shape)\n",
    "print('weights 2 shape', weights_matrix2[1].shape)\n",
    "print('weights 3 shape', weights_matrix2[2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "---\n",
    "***The intuition behind the backpropagation algorithm is as follows:***\n",
    "\n",
    "* Given a training example $(x^{(t)},y^{(t)})$, we will first run a âforward passâ to compute all the activations throughout the network, including the output value of the hypothesis $h_Î(x)$. \n",
    "\n",
    "* Then, for each node $j$ in layer $l$, we would like to compute an âerror termâ $\\delta_j^{(l)}$ that measures how much that node was âresponsibleâ for any errors in our output. For an output node, we can directly measure the difference between the networkâs activation and the true target value, and use that to define $\\delta_j^{(output)}$. For the hidden units, you will compute $\\delta_j^{(l)}$ based on a weighted average of the error terms of the nodes in layer $(l + 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivation of the Sigmoid function (for calculating gradient of the cost function in backpropagation)\n",
    "---\n",
    "1. Sigmoid function: $ g(x) = \\frac{1}{(1+ e^{-x})} $\n",
    "<br><br>\n",
    "2. $ g(x) = \\frac{1}{(1+ e^{-x})} = (1+e^{-x})^{-1}$\n",
    "<br><br>\n",
    "\n",
    "3. Apply Chain Rule: $ g'(x) = -1\\times(1+e^{-x})^{-2}\\times(1+e^{-x})' $\n",
    "<br><br>\n",
    "4. Apply Exponential Rule $ g'(x) = -(1+e^{-x})^{-2}\\times(e^{-x}\\times(-x)') $\n",
    "<br><br>\n",
    "5. $ g'(x) = -(1+e^{-x})^{-2}\\times(e^{-x}\\times-1) $\n",
    "<br><br>\n",
    "6. $ g'(x) = \\frac{1}{(1+e^{-x})^{2}}\\times e^{-x} $\n",
    "<br><br>\n",
    "7.  $ g'(x) = \\frac{1}{(1+e^{-x})}\\times\\frac{e^{-x}}{{(1+e^{-x})}} $\n",
    "<br><br>\n",
    "8.  $ g'(x) = g(x)\\times\\frac{1+e^{-x}-1}{{(1+e^{-x})}} $\n",
    "<br><br>\n",
    "9.  $ g'(x) = g(x)\\times(1-g(x)) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivative of the Sigmoid function \n",
    "def sigmoid_gradient(X):\n",
    "    return sigmoid(X)*(1-sigmoid(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check sigmoid_gradient \n",
    "sigmoid_gradient(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implement the backpropagation algorithm to compute the gradients of weights to obtain \n",
    "the gradient for the neural network cost function. \n",
    "\n",
    "This function returns the partial derivatives of the cost function with respect to the weights in each hidden layer, \n",
    "for later use in fmin_cg function to minimize cost.\n",
    "\"\"\"\n",
    "\n",
    "def backpropagation(vec_weights, hidden_layer_sizes, n_labels, X, y, alpha):\n",
    "    \n",
    "    reg_cost, y_prob, y_pred, activations_x, weights = feedforward(vec_weights, \n",
    "                                                                   hidden_layer_sizes, \n",
    "                                                                   n_labels, \n",
    "                                                                   X, y, alpha)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    n_activations = len(activations_x)\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    # Turn y into dummy index \n",
    "    y = pd.get_dummies(y.astype('category'))\n",
    "    y = np.array(y, dtype = np.int64)\n",
    "    \n",
    "    # Compute 'error term' delta of the output layer\n",
    "    last_layer_weights_delta = y_prob - y\n",
    "    \n",
    "    # Initialze a list of deltas from each hidden layer\n",
    "    layer_weights_delta = []\n",
    "    \n",
    "    # Compute 'error term' delta of the hidden layers\n",
    "    for n in reversed(range(n_activations)):\n",
    "        delta = np.dot(last_layer_weights_delta.T, activations_x[n])\n",
    "        avg_delta = delta/n_samples\n",
    "\n",
    "        # Compute regularization of the corresponding weights_grad\n",
    "        weights_temp = weights[n].copy()\n",
    "\n",
    "        # Replace the first column of weights with 0 to leave out bias column in regularized cost\n",
    "        weights_temp[:,1] = 0\n",
    "        reg_term = (alpha/n_samples)*weights_temp.copy()\n",
    "\n",
    "        # Update weights_grad with regularization term \n",
    "        layer_grad_reg = avg_delta+reg_term\n",
    "        print('Theta gradient {} shape: '.format(n+1), layer_grad_reg.shape)\n",
    "        \n",
    "        # Unroll delta and insert it as the first item in weights_gradient list \n",
    "        layer_weights_delta.insert(0, np.concatenate(layer_grad_reg))\n",
    "        \n",
    "        # Update last_layer_weights_delta\n",
    "        last_layer_weights_delta = np.dot(last_layer_weights_delta, \n",
    "                                          weights[n][:,1:])*sigmoid_gradient(activations_x[n][:,1:])\n",
    "        \n",
    "    return np.concatenate(layer_weights_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta gradient 4 shape:  (10, 9)\n",
      "Theta gradient 3 shape:  (8, 17)\n",
      "Theta gradient 2 shape:  (16, 9)\n",
      "Theta gradient 1 shape:  (8, 65)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(890,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test backpropagation with feedforwd test results\n",
    "weights_grad = backpropagation(vec_weights = weights_vector2, \n",
    "                                        hidden_layer_sizes = [8, 16, 8],\n",
    "                                        n_labels=10, \n",
    "                                        X=X_train, y = y_train,\n",
    "                                        alpha = 1)\n",
    "weights_grad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put everything together \n",
    "---\n",
    "***Put all the functions we build so far together, as one function that computes the regularized Neural Network model cost and weights gradient***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to minize that returns the cost\n",
    "def nn_cost(vec_weights, *args):\n",
    "    \n",
    "    hidden_layer_sizes, n_labels, X, y, alpha = args\n",
    "    reg_cost, y_prob, predictions, activations_x, weights = feedforward(vec_weights, \n",
    "                                                                        hidden_layer_sizes,\n",
    "                                                                        n_labels, \n",
    "                                                                        X, y, \n",
    "                                                                        alpha)\n",
    "    \n",
    "    return reg_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a function to minize that returns the weights cost gradient\n",
    "def nn_cost_gradient(vec_weights, *args):  \n",
    "    hidden_layer_sizes, n_labels, X, y, alpha = args\n",
    "    return backpropagation(vec_weights, \n",
    "                                       hidden_layer_sizes, \n",
    "                                       n_labels, \n",
    "                                       X, y, \n",
    "                                       alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of weights in matrix:  4\n",
      "Theta gradient 2 shape:  (10, 9)\n",
      "Theta gradient 1 shape:  (8, 65)\n",
      "2.945120500303429\n"
     ]
    }
   ],
   "source": [
    "# Initialize new weights with rand_weights_init and test nn_cost, nn_cost_gradient\n",
    "test_weights_m, test_weights_vec = rand_weights_init(X_train, 10, [8,16,8])\n",
    "\n",
    "args = ([8], 10, X_train, y_train, 0)\n",
    "cost = nn_cost(test_weights_vec, *args)\n",
    "grad = nn_cost_gradient(test_weights_vec, *args)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimize the nn_cost function using a nonlinear conjugate gradient algorithm\n",
    "--- \n",
    "\n",
    "***Using the function fmin_cg from SciPy library to minimize our cost function***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_cg\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of weights in matrix:  2\n"
     ]
    }
   ],
   "source": [
    "weights_1_m, weights_1_v = rand_weights_init(X_train, 10, [8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.644364\n",
      "         Iterations: 100\n",
      "         Function evaluations: 110772\n",
      "         Gradient evaluations: 181\n"
     ]
    }
   ],
   "source": [
    "# Using nn_cost \n",
    "optim_weights = fmin_cg(f = nn_cost, x0 = weights_1_v, args = args, maxiter = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6443644765515987"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_cost(optim_weights, *args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_cost_optm, y_prob_optm, predictions_optm, activations_x_optm, weights_optm = feedforward(optim_weights, \n",
    "                                                                        [8],\n",
    "                                                                        10, \n",
    "                                                                        X_train, y_train, \n",
    "                                                                        alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = (predictions_optm == y_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.649025069637883"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
